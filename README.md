# 生成文本评价指标（Generated Text Evaluation Metrics）
### **总结：各指标对比**
| 指标       | 核心方法               | 优势                     | 局限性                     | 典型场景           |
|------------|------------------------|--------------------------|----------------------------|--------------------|
| BLEU       | n-gram重叠度           | 快速、通用               | 忽略语义、依赖参考译文     | 机器翻译           |
| ROUGE      | n-gram/LCS召回率       | 适合摘要的结构匹配       | 冗余倾向、表面匹配         | 文本摘要           |
| METEOR     | 同义词+词干匹配        | 语义相关性更强           | 依赖词典、计算慢           | 多语言翻译         |
| chrF       | 字符级n-gram           | 适用于形态复杂语言       | 忽略词级语义               | 中文、俄语翻译     |
| CIDEr      | TF-IDF加权n-gram       | 突出共识词重要性         | 依赖多参考描述             | 图像描述           |
| BERTScore  | 预训练模型语义向量     | 深层语义匹配             | 计算耗时、依赖模型         | 多任务补充评价     |

### **1. BLEU（Bilingual Evaluation Understudy）**
- **核心原理**：通过**比较机器翻译结果与人工翻译**参考译文（通常1-4句）的**n-gram重叠度**（连续n个词的匹配）来评分，是应用最广泛的机器翻译评价指标。
  - 计算步骤：
    1. 统计翻译结果中与参考译文匹配的1-gram（单字）、2-gram（双字）…4-gram（四字）的数量。
    2. 对长于参考译文的n-gram匹配进行惩罚（避免“堆砌高频词”）。
    3. 对不同n-gram的得分取几何平均，并乘以长度惩罚因子，最终得分范围为0-1（通常乘以100表示百分比）。

- **优点**：
  - 计算快速、可复现性强，与人工评价相关性较高。
  - 适用于多种语言对，无需语言学专业知识。

- **缺点**：
  - 仅关注词的**表面匹配，忽略语义、语法正确性**。
  - 依赖参考译文数量（参考译文越少，评分偏差越大）。
  - 对短句评分波动较大。

- **适用场景**：机器翻译模型快速迭代、大规模评测（如WMT竞赛）。


### **2. ROUGE（Recall-Oriented Understudy for Gisting Evaluation）**
- **核心原理**：专为**文本摘要**设计，侧重召回率（生成摘要包含参考摘要中n-gram的比例），常用变体包括ROUGE-N（n-gram）、ROUGE-L（最长公共子序列LCS）等。
  - 例如ROUGE-L：通过计算生成摘要与参考摘要的最长公共子序列长度，衡量语义连贯性。

- **优点**：
  - 更适合摘要任务，能捕捉**句子级的结构相似性**（如ROUGE-L对语序不敏感）。

- **缺点**：
  - 召回率导向可能导致生成冗余内容；仍依赖**表面匹配，忽略语义深度**。

- **适用场景**：文本摘要、自动问答的答案生成评价。


### **3. METEOR（Metric for Evaluation of Translation with Explicit ORder）**
- **核心原理**：在BLEU基础上优化，结合**同义词匹配**（通过WordNet等词典）、词干还原（如“running”与“run”视为匹配）和语序调整惩罚，更注重语义相关性。
  - 计算步骤：先统计精确匹配的n-gram，再扩展到同义词、词干匹配，最后通过chunk（连续匹配序列）的数量惩罚语序混乱。

- **优点**：
  - 比BLEU更接近人工评价，能处理**同义词和形态变化**（如英语时态、法语阴阳性）。

- **缺点**：
  - **计算复杂度高**于BLEU，**依赖外部词典**（多语言支持有限）。
  - 对长句的语序惩罚可能过于严格。

- **适用场景**：对语义匹配要求较高的翻译任务（如文学翻译）。


### **4. chrF（Character n-gram F-score）**
- **核心原理**：基于**字符级n-gram**（而非词级），适用于**形态丰富的语言**（如德语、芬兰语）或无明显词边界的语言（如中文、日语）。
  - 例如中文中“机器学习”拆分为字符n-gram（“机”“器”“学”“习”“机器”“器学”等），通过字符匹配衡量相似度。

- **优点**：
  - 解决词分割（tokenization）难题，对拼写错误更敏感（如“teh”与“the”的1-gram字符匹配度为2/3）。

- **缺点**：
  - 字符级匹配可能**忽略词的整体语义**（如“苹果”和“芒果”字符重叠低，但均为水果）。

- **适用场景**：形态复杂语言（如俄语）、无显式词边界语言（如中文、泰语）的翻译评价。


### **5. CIDEr（Consensus-based Image Description Evaluation）**
- **核心原理**：专为**图像描述** 设计，通过计算生成描述与人工描述的TF-IDF加权n-gram相似度，突出“共识词”（被多个参考描述使用的词）的重要性。

- **优点**：
  - **考虑词的重要性**（如“猫”在猫的图片描述中权重更高），与人工评价相关性强。

- **缺点**：
  - 依赖大量**参考描述**，计算成本较高。

- **适用场景**：图像描述生成、视频字幕评价。


### **6. BERTScore**
- **核心原理**：**基于预训练语言模型**（如BERT）的**语义向量相似度**，通过计算生成文本与参考文本中**词的上下文嵌入（embedding）余弦相似度**，衡量深层语义匹配。

- **优点**：
  - 突破表面匹配限制，能捕捉同义词、 paraphrase（如“汽车”和“轿车”）的**语义等价性**。
  - 在多种任务（翻译、摘要、对话）中表现优于传统指标。

- **缺点**：
  - 计算耗时（需加载大型预训练模型）；对模型选择和语言敏感（如多语言BERT在小语种上表现可能下降）。

- **适用场景**：对语义准确性要求高的任务，或作为传统指标的补充。

